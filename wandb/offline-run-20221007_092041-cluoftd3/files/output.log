
previous best checkpoint: epoch:0, loss:0.4731909034162527
You are using a model of type visual_bert to instantiate a model of type lxmert. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|                                                                                                                                          | 0/110 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/content/SSLMemes/src/train.py", line 309, in <module>
    step_global=global_step, epoch=epoch, val_best_score=val_best_score, processor=processor)
  File "/content/SSLMemes/src/train.py", line 82, in train
    scaler.step(optimizer)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 339, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 286, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py", line 121, in step
    group['eps'])
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py", line 128, in adamw
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!