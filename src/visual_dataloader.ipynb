{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, LxmertTokenizer\n",
    "from data import ImageTextClassificationDataset, collate_fn_batch_visualbert, collate_fn_batch_lxmert, collate_fn_batch_visualbert_semi_supervised, collate_fn_batch_lxmert_semi_supervised\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "img_feature_path              ../data/features/visualgenome/\n",
       "train_csv_path      ../data/splits/random/memotion_train.csv\n",
       "val_csv_path          ../data/splits/random/memotion_val.csv\n",
       "model_type                                        visualbert\n",
       "model_path                   uclanlp/visualbert-vqa-coco-pre\n",
       "learning_rate                                        0.00002\n",
       "epoch                                                    100\n",
       "eval_step                                                100\n",
       "batch_size                                                64\n",
       "amp                                                     True\n",
       "output_dir                                             ./tmp\n",
       "checkpoint_step                                         1000\n",
       "random_seed                                               42\n",
       "resume_training                                        False\n",
       "semi_supervised                                        False\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='train')\n",
    "parser = pd.Series()\n",
    "parser = parser.append(pd.Series({'img_feature_path': \"../data/features/visualgenome/\"}))\n",
    "parser = parser.append(pd.Series({'train_csv_path': \"../data/splits/random/memotion_train.csv\"}))\n",
    "parser = parser.append(pd.Series({'val_csv_path': \"../data/splits/random/memotion_val.csv\"}))\n",
    "parser = parser.append(pd.Series({'model_type': \"visualbert\"}))\n",
    "parser = parser.append(pd.Series({'model_path': \"uclanlp/visualbert-vqa-coco-pre\"}))\n",
    "parser = parser.append(pd.Series({'learning_rate': 2e-5}))\n",
    "parser = parser.append(pd.Series({'epoch': 100}))\n",
    "parser = parser.append(pd.Series({'eval_step': 100}))\n",
    "parser = parser.append(pd.Series({'batch_size': 64}))\n",
    "parser = parser.append(pd.Series({'amp':True}))\n",
    "parser = parser.append(pd.Series({'output_dir': \"./tmp\"}))\n",
    "parser = parser.append(pd.Series({'checkpoint_step': 1000}))\n",
    "parser = parser.append(pd.Series({'random_seed': 42}))\n",
    "parser = parser.append(pd.Series({'resume_training': False}))\n",
    "parser = parser.append(pd.Series({'semi_supervised': False}))\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_path = args.img_feature_path\n",
    "model_type = args.model_type\n",
    "# dataset_train = ImageTextClassificationDataset(img_feature_path, args.train_csv_path, \n",
    "#             supervise = not args.semi_supervised,model_type=model_type, vilt_processor=processor,mode='train')\n",
    "dataset_train = ImageTextClassificationDataset(img_feature_path, args.val_csv_path, model_type=model_type,mode='train', \n",
    "                                                debug=True, metadata_path='../data/features/visualgenome/train_images/metadata.json'\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "if model_type == \"visualbert\":\n",
    "    # config = VisualBertConfig.from_pretrained(args.model_path)\n",
    "    # model = VisualBertModel.from_pretrained(args.model_path)\n",
    "    # model = ModelForBinaryClassification(model,config)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    # processor = None\n",
    "elif model_type == \"lxmert\":\n",
    "    # config = LxmertConfig.from_pretrained(args.model_path)\n",
    "    # model = LxmertModel.from_pretrained(args.model_path)\n",
    "    # model = ModelForBinaryClassification(model,config)\n",
    "    tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\") \n",
    "    # processor = None\n",
    "# elif model_type == \"vilt\":\n",
    "#     from transformers import ViltProcessor, ViltModel, ViltForImagesAndTextClassification\n",
    "#     config = AutoConfig.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "#     config.num_images = 1\n",
    "#     model = ViltForImagesAndTextClassification(config)\n",
    "#     model.vilt = ViltModel.from_pretrained(args.model_path)\n",
    "#     processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "#     tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.semi_supervised:\n",
    "    if model_type == \"visualbert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_visualbert_semi_supervised,tokenizer=tokenizer)\n",
    "    elif model_type == \"lxmert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_lxmert_semi_supervised,tokenizer=tokenizer)\n",
    "else:\n",
    "    if model_type == \"visualbert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_visualbert,tokenizer=tokenizer, debug=True)\n",
    "    elif model_type == \"lxmert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_lxmert,tokenizer=tokenizer)\n",
    "    # elif model_type == \"vilt\":\n",
    "    #     collate_fn_batch = partial(collate_fn_batch_vilt,processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    collate_fn = collate_fn_batch,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_toks, batch_img_features, batch_labels, batch_metadata = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'at', '3a', '##m', 'watching', 'info', '##mer', '##cial', '##s', '1844', '##9', '##41', '##7', 'mozambique', 'pigeon', 'blood', 'red', 'hub', '##y', 'di', '##omo', '##nd', 'ring', 'gr', '##sg', '##iano', 'heat', '512', '##ct', 'center']\n",
      "['can', 'u', 'imagine', 'how', 'hot', 'id', 'be', 'if', 'i', 'ate', 'right', 'and', 'took', 'care', 'of', 'my', 'body', 'im', 'not', 'gonna', 'do', 'it', 'but', 'can', 'u', 'imagine']\n",
      "['when', 'its', '2', 'am', 'but', 'you', 'can', '##t', 'stop', 'thinking', 'about', 'that', 'time', 'you', 'tripped', 'during', 'the', '4th', 'grade', 'spelling', 'bee']\n",
      "['sorting', 'by', 'new', 'and', 'only', 'seeing', 'rep', '##ost', '##s', 'anime', '##s', '95', 'puck', '##you', 'fuck', 'you', 'too']\n",
      "['people', 'hearing', 'im', 'a', 'cop', 'people', 'realizing', 'im', 'just', 'a', 'mall', 'cop', 'mg', '##fi', '##pc', '##om']\n",
      "['me', 'pulls', 'out', 'a', 'pack', 'of', 'gum', 'the', 'entire', 'fucking', 'class']\n",
      "['me', 'reading', 'all', 'the', 'kind', 'comments', 'red', '##dit', '##ors', 'leave', 'on', 'my', 'me', '##me']\n",
      "['space', '##x', 'nasa', 'live', 'video', 'of', 'the', 'curvature', 'of', 'the', 'earth', 'flat', 'earth', '##ers']\n",
      "['americans', '18', '##yo', 'son', 'dad', 'me', 'and', 'my', 'friends', 'bought', 'a', 'six', 'pack', 'and', 'got', 'wasted', 'in', 'a', 'parking', 'lot', 'and', 'now', 'i', 'feel', 'sick', 'no', '##o', 'your', '##e', 'not']\n",
      "['remember', 'gordon', 'ramsay', 'this', 'is', 'him', 'now', 'feel', 'old', 'yet']\n",
      "['french', '##cana', '##dian', '##s', 'after', 'being', 'sent', 'to', 'fight', 'in', 'the', 'ale', '##uti', '##an', 'campaign', 'bea', '##ca', '##use', 'its', '##tech', '##nical', '##l', 'not', '##overs', '##ea']\n",
      "['me', 'and', 'my', 'five', 'brain', 'cells', 'preparing', 'to', 'say', 'here', 'when', 'the', 'teacher', 'calls', 'attendance']\n",
      "['interview', '##er', 'do', 'you', 'have', 'any', 'experience', 'as', 'a', 'surgeon', 'Ð¼', '##e']\n",
      "['me', 'after', 'training', 'at', 'home', 'for', '43', '##2', 'minutes', 'i', 'have', 'to', 'rest', '22', 'hours', 'a', 'day']\n",
      "['americans', 'when', 'they', 'visit', 'vietnam', 'who', 'would', 'have', 'thought', 'che', 'al', 'these', 'years', 'v', '##d', 'return', 'to', 'the', 'scene', 'of', 'my', 'great', '##ea', '##t', 'military', 'da', '##gra', '##ce']\n",
      "['relationship', 'status', 'single', 'taken', 'v', 'too', 'anxious', 'to', 'ever', 'make', 'a', 'move']\n",
      "['phi', '##neas', 'and', 'fe', '##rb', '##s', 'inventions', 'click', 'el', '##on', 'mu', '##sk']\n",
      "['alexander', 'graham', 'bell', 'in', '##vent', '##s', 'telephone', 'alexander', 'graham', 'bell', 'dies', 'his', 'mom', 'its', 'because', 'of', 'that', 'damn', 'phone', 'mum', 'pl', '##s']\n",
      "['friend', 'do', 'u', 'want', 'some', 'eye', 'drops', 'me', 'nah', 'l', '##m', 'good', 'also', 'me']\n",
      "['po', '##v', 'you', 'work', 'at', 'netflix']\n",
      "['me', 'moving', 'my', 'foot', 'under', 'the', 'blanket', 'my', 'cat', 'finally', '##aw', '##orth', '##y', 'opponent', 'our', 'battle', 'will', 'be', 'legendary']\n",
      "['kam', '##me', '##ein']\n",
      "['me', 'getting', 'my', 'gym', 'partner', 'ready', 'for', 'their', 'set']\n",
      "['were', 'going', 'to', 'the', 'beach', 'don', '##t', 'forget', 'to', 'pack', 'the', 'essential', '##s', 'me']\n",
      "['wi', '##fi', 'drops', 'down', 'by', '1', 'bar', 'youtube', 'video', 'quality']\n",
      "['me', 'does', 'nothing', 'me', 'why', 'is', 'life', 'so', 'hard', 'now']\n",
      "['when', 'all', 'i', 'want', 'is', 's', '##nu', '##ggles', 'but', 'all', 'i', 'keep', 'getting', 'is', 'struggles']\n",
      "['guy', 'leaving', 'for', 'war', 'her', 'don', '##t', 'talk', 'to', 'other', 'girls', 'over', 'there', 'sarcasm']\n",
      "['i', 'thought', 'today', 'was', 'friday', 'its', 'only', 'wednesday']\n",
      "['how', 'i', 'clean', 'my', 'plate', 'when', 'people', 'are', 'around', 'how', 'i', 'clean', 'my', 'plate', 'when', 'im', 'by', 'myself']\n",
      "['me', 'and', 'my', 'five', 'brain', 'cells', 'preparing', 'to', 'say', 'here', 'when', 'the', 'teacher', 'calls', 'attendance']\n",
      "['when', 'the', 'barber', 'is', 'cutting', 'your', 'hair', 'shorter', 'than', 'you', 'want', 'but', 'your', '##e', 'too', 'shy', 'to', 'complain', 'so', 'you', 'just', 'sit', 'there', 'like']\n",
      "['person', 'in', 'ad', 'yo', '##yl', '##l', 'probably', 'skip', 'this', 'but', 'your', '##e', 'goddamn', 'right']\n",
      "['when', 'your', 'mom', 'would', 'tell', 'you', 'to', 'cover', 'your', 'eyes', 'during', 'a', 'sex', 'scene', 'in', 'a', 'movie', '0']\n",
      "['when', 'your', '##e', 'in', 'the', 'elevator', 'and', 'press', 'the', 'button', 'while', 'there', 'is', 'people', 'approaching']\n",
      "['whenever', 'i', 'see', 'a', 'post', '##of', 'people', 'doing', 'extremely', 'stupid', 'shit', 'i', 'tell', 'myself', 'its', 'fake', 'nobody', 'can', 'be', 'that', 'stupid', 'then', 'i', 'remember', 'that', 'americans', 'exist']\n",
      "['the', 'four', 'horse', '##man', 'of', 'slowing', 'down', 'time', 'holding', 'your', 'breath', 'plank', 'ads', 'micro', '##wa', '##ving', 'food']\n",
      "['them', 'not', 'everything', 'has', 'to', 'be', 'an', 'office', 'reference', 'me', 'win', 'sr', 'why', 'are', 'you', 'the', 'way', 'that', 'you', 'are']\n",
      "['w', '##w', '##1', 'ends', 'senators', 'stabbing', 'caesar', 'caesar', 'my', 'ho', '##mie', 'br', '##utus', 'will', 'save', 'me', 'the', 'spanish', 'flu', 'br', '##utus', 'hey', 'fell', '##as', 'no', 'i', 'don', '##t', 'think', 'i']\n",
      "['sending', 'pictures', 'to', 'your', 'bf', '##f', 'to', 'your', 'crush']\n",
      "['just', 'a', 'tiny', 'far', '##t', 'nobody', 'will', 'hear', 'it', 'ass']\n",
      "['bf', '##f', 'lets', 'go', 'on', 'a', 'diet', 'together', 'me', 'om', '##g', 'yes', '5', 'min', '##s', 'later']\n",
      "['cat', '##ur', '##day', 'is', '5', 'days', 'away', 'sadly', 'tad', '##e', 'on', 'it', '##ig', '##ur']\n",
      "['when', 'you', 'commit', 'your', 'changes', 'to', 'a', 'branch', 'but', 'forgot', 'to', 'pull', 'before', 'you', '##star', '##ted', 'working']\n",
      "['the', 'ne', '##rdy', 'kid', 'in', 'highs', '##cho', '##ol', 'the', 'popular', 'kid', 'in', 'highs', '##cho', '##ol', 'smoking', 'way', 'too', 'much', 'weed', 'in', 'college']\n",
      "['not', 'even', 'thor', 'is', 'worthy', 'to', 'move', 'the', 'kitty', '77', '##11', '##11']\n",
      "['danielle', 'him', 'stop', 'acting', 'dumb', 'me']\n",
      "['office', 'sp', '##ut', 'count', 'judge', 'mind', '##y', 's', 'g', '##laze', '##r', 'presiding', '05', '112', '##0', '##21', '09', '##53', '##53', '28', '##yr', '##old', 'dressed', 'as', 'teen', 'sneak', '##s', 'into', 'us', 'school']\n",
      "['when', 'your', 'son', 'is', 'going', 'through', 'a', 'phase', 'ada', '##bm', '##oms']\n",
      "['game', 'of', 'throne', '##s', 'writers', 'when', 'they', 'ran', 'out', 'of', 'source', 'material', 'there', '##s', 'something', 'so', 'human', 'about', 'taking', 'something', 'great', 'and', 'ruining', 'it', 'a', 'little', 'so', 'you', 'can', 'have']\n",
      "['disney', 'our', 'new', 'vr', 'game', 'will', 'let', 'you', 'live', 'out', 'your', 'lights', '##abe', '##r', 'fantasy', 'me']\n",
      "['when', 'you', '##ve', 'had', 'it', 'with', 'their', 'nonsense']\n",
      "['when', 'your', '##e', 'hungry', 'but', 'all', 'the', 'food', 'in', 'the', 'house', 'needs', 'to', 'be', 'cooked']\n",
      "['google', 'mein', 'kam', '##p', 'made', '##s', 'ah', '##o', 'wo', '##ce', 'mein', 'kam', '##pf', 'ao', '##ok', 'by', 'asa', '##t', 'er', 'o', '##ve', '##mme', '##w', 'rev', '##ew', '##s', 'er', 'no', '##ok', 'more']\n",
      "['when', 'someone', 'tells', 'you', 'a', 'dark', 'joke', 'and', 'you', 'get', 'the', 'green', 'light', 'to', 'start', 'telling', 'all', 'your', 'own', 'dark', 'jokes']\n",
      "['when', 'my', 'best', 'friend', 'and', 'i', 'are', 'having', 'a', 'conversation', 'and', 'the', 'teacher', 'interrupt', '##s', 'us']\n",
      "['when', 'your', 'hot', '##line', 'b', '##ling', 'but', 'its', 'just', 'your', 'service', 'provider', 'telling', 'you', 'your', '##e', 'almost', 'out', 'of', 'data']\n",
      "['there', '##s', 'always', 'that', 'one', 'kid', 'ruining', 'every', 'family', 'photo']\n",
      "['mom', 'don', '##t', 'worry', 'the', 'food', 'is', 'not', 'that', 'hot', 'the', 'food']\n",
      "['ii', '##zin', '##urs', '##pace', '##ime', '##con', '##tin', '##u', '##um', '##ups', '##etti', '##ng', 'all', 'your', 'gravity', '##and', 'quantum', '##san', '##d', 'stu', '##ft', '##s']\n",
      "['got', 'suspended', 'twice', 'in', 'school', 'suspension', '5', 'times', 'always', 'skipped', 'classes', 'had', 'a', '24', 'gp', '##a', 'and', 'did', 'nothing', 'but', 'fuck', 'around', 'u', 'can', 'do', 'it', 'kids']\n",
      "['you', '##ll', 'face', 'off', 'against', '##a', 'villain', 'and', 'die', 'an', 'un', '##sp', '##eak', '##ably', 'gr', '##ues', '##ome', 'death', 'nm', '##or']\n",
      "['gentlemen', 'it', 'is', 'with', 'great', 'pleasure', 'to', 'inform', 'you', 'that', 'today', 'is', 'wednesday']\n",
      "['the', 'left', 'the', 'right', 'me', 'knowing', 'neither', 'side', 'will', 'actually', 'do', 'anything', 'to', 'fix', 'our', 'problems', 'so', 'we', 'might', 'as', 'well', 'find', 'a', 'good', 'fish', '##in', 'hole', 'and', 'watch', 'it']\n"
     ]
    }
   ],
   "source": [
    "for caption_ids in batch_toks['input_ids'].tolist():\n",
    "    print(tokenizer.convert_ids_to_tokens(caption_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [0, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
