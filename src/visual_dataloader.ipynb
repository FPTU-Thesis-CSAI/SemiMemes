{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, LxmertTokenizer\n",
    "from data import ImageTextClassificationDataset, collate_fn_batch_visualbert, collate_fn_batch_lxmert, collate_fn_batch_visualbert_semi_supervised, collate_fn_batch_lxmert_semi_supervised\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "img_feature_path                ../data/features/visualbert/\n",
       "train_csv_path      ../data/splits/random/memotion_train.csv\n",
       "val_csv_path          ../data/splits/random/memotion_val.csv\n",
       "model_type                                        visualbert\n",
       "model_path                 uclanlp/visualbert-nlvr2-coco-pre\n",
       "learning_rate                                        0.00002\n",
       "epoch                                                    100\n",
       "eval_step                                                100\n",
       "batch_size                                                64\n",
       "amp                                                     True\n",
       "output_dir                                             ./tmp\n",
       "checkpoint_step                                         1000\n",
       "random_seed                                               42\n",
       "resume_training                                        False\n",
       "semi_supervised                                        False\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='train')\n",
    "parser = pd.Series()\n",
    "parser = parser.append(pd.Series({'img_feature_path': \"../data/features/visualbert/\"}))\n",
    "parser = parser.append(pd.Series({'train_csv_path': \"../data/splits/random/memotion_train.csv\"}))\n",
    "parser = parser.append(pd.Series({'val_csv_path': \"../data/splits/random/memotion_val.csv\"}))\n",
    "parser = parser.append(pd.Series({'model_type': \"visualbert\"}))\n",
    "parser = parser.append(pd.Series({'model_path': \"uclanlp/visualbert-nlvr2-coco-pre\"}))\n",
    "parser = parser.append(pd.Series({'learning_rate': 2e-5}))\n",
    "parser = parser.append(pd.Series({'epoch': 100}))\n",
    "parser = parser.append(pd.Series({'eval_step': 100}))\n",
    "parser = parser.append(pd.Series({'batch_size': 64}))\n",
    "parser = parser.append(pd.Series({'amp':True}))\n",
    "parser = parser.append(pd.Series({'output_dir': \"./tmp\"}))\n",
    "parser = parser.append(pd.Series({'checkpoint_step': 1000}))\n",
    "parser = parser.append(pd.Series({'random_seed': 42}))\n",
    "parser = parser.append(pd.Series({'resume_training': False}))\n",
    "parser = parser.append(pd.Series({'semi_supervised': False}))\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_path = args.img_feature_path\n",
    "model_type = args.model_type\n",
    "# dataset_train = ImageTextClassificationDataset(img_feature_path, args.train_csv_path, \n",
    "#             supervise = not args.semi_supervised,model_type=model_type, vilt_processor=processor,mode='train')\n",
    "dataset_train = ImageTextClassificationDataset(img_feature_path, args.val_csv_path, model_type=model_type,mode='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "if model_type == \"visualbert\":\n",
    "    # config = VisualBertConfig.from_pretrained(args.model_path)\n",
    "    # model = VisualBertModel.from_pretrained(args.model_path)\n",
    "    # model = ModelForBinaryClassification(model,config)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    # processor = None\n",
    "elif model_type == \"lxmert\":\n",
    "    # config = LxmertConfig.from_pretrained(args.model_path)\n",
    "    # model = LxmertModel.from_pretrained(args.model_path)\n",
    "    # model = ModelForBinaryClassification(model,config)\n",
    "    tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\") \n",
    "    # processor = None\n",
    "# elif model_type == \"vilt\":\n",
    "#     from transformers import ViltProcessor, ViltModel, ViltForImagesAndTextClassification\n",
    "#     config = AutoConfig.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "#     config.num_images = 1\n",
    "#     model = ViltForImagesAndTextClassification(config)\n",
    "#     model.vilt = ViltModel.from_pretrained(args.model_path)\n",
    "#     processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "#     tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.semi_supervised:\n",
    "    if model_type == \"visualbert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_visualbert_semi_supervised,tokenizer=tokenizer)\n",
    "    elif model_type == \"lxmert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_lxmert_semi_supervised,tokenizer=tokenizer)\n",
    "else:\n",
    "    if model_type == \"visualbert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_visualbert,tokenizer=tokenizer)\n",
    "    elif model_type == \"lxmert\":\n",
    "        collate_fn_batch = partial(collate_fn_batch_lxmert,tokenizer=tokenizer)\n",
    "    # elif model_type == \"vilt\":\n",
    "    #     collate_fn_batch = partial(collate_fn_batch_vilt,processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    collate_fn = collate_fn_batch,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_toks, batch_img_features, batch_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'sure', 'if', 'this', 'pain', 'is', 'new', 'or', 'if', 'im', 'just', 'now', 'noticing']\n",
      "['daughter', 'i', 'want', 'a', 'dinosaur', 'dad', 'its', 'impossible', 'daughter', 'then', 'let', 'me', 'have', 'a', 'boyfriend', 'dad']\n",
      "['1000', 'phone', '99', 'cent', 'app']\n",
      "['dad', '##do', '##es', '##nt', 'want', 'dog', 'meg', '##ets', 'a', 'dog', 'anyway', '##s', 'dad', 'and', 'the', 'dog']\n",
      "['when', 'your', 'favorite', 'teacher', 'yells', 'at', 'you', 'the', 'sad', 'thing', 'is', 'ia', '##ct', '##ually', 'thought', 'you', 'were', 'different']\n",
      "['ok', '##ne', '##w', 'dub', '##ste', '##p', 'was', 'invented', 'by', 'nazis']\n",
      "['on', 'your', 'way', 'to', 'the', 'gym', 'vs', 'after', 'training', 'legs', 'for', 'two', 'hours']\n",
      "['when', 'ur', 'joke', 'ruins', 'the', 'conversation', 'but', 'you', 'know', 'it', 'was', 'fire']\n",
      "['teacher', 'cm', '##on', 'guys', 'you', 'learned', 'this', 'last', 'year', 'me', 'trying', 'to', 'remember', 'what', 'i', 'ate', 'for', 'breakfast']\n",
      "['me', 'cutting', 'back', 'the', 'hedge', 'all', 'the', 'other', 'vietnamese', 'soldiers']\n",
      "['when', 'you', 'get', 'called', 'into', 'work', 'on', 'your', 'day', 'off', 'o', 'du', '##m', '##ng']\n",
      "['my', 'gut', 'bacteria', 'watching', 'the', 'po', '##op', 'depart']\n",
      "['plague', 'inc', 'dev', '##ol', '##pers', 'should', 'add', 'a', 'karen', 'option', 'that', 'will', 'help', 'spread', 'the', 'virus', 'and', 'slow', '##s', 'the', 'vaccines', 'and', 'cure']\n",
      "['mini', '##ski', '##rts', 'change', 'his', 'mind', 'please']\n",
      "['girl', 'please', 'god', 'send', 'me', 'a', 'good', 'man', 'god', 'sends', 'one', 'girl', 'e', '##w', 'not', 'this', 'one', 'he', 'is', '59', 'god']\n",
      "['how', 'im', '##a', 'lose', 'weight', 'if', 'everything', 'taste', 'good', 'lam']\n",
      "['how', 'i', 'drink', 'water', 'in', 'the', 'kitchen', 'at', 'midnight', 'looking', 'out', 'for', 'ghosts', 'and', 'evil', 'spirits']\n",
      "['i', 'love', 'your', 'won', '##k', 'st', '##al', '##n', 'ra', '##dt', '##het', 'raid', 'ni', '##ll', '##be', '##f', '##hen', '##the', '##yse', '235', '##9', '000', '##0']\n",
      "['when', 'you', '20', 'min', '##s', 'deep', 'in', 'an', 'argument', 'and', 'realize', 'you', 'misunderstood', 'something']\n",
      "['when', 'you', 'see', 'halloween', 'decorations', 'still', 'up', 'on', 'november', '1', 'do', 'they', 'know', 'its', 'christmas', 'time', 'at', 'all']\n",
      "['me', 'to', 'the', 'silent', 'kid', 'in', 'the', 'class', 'i', 'really', 'wanna', 'be', 'ur', 'friend', 'but', 'i', 'have', 'no', 'idea', 'how', 'to', 'start', 'a', 'conversation']\n",
      "['my', 'entire', 'team', 'in', 'online', 'multiplayer', 'some', 'chinese', 'kid']\n",
      "['when', 'two', 'of', 'your', 'siblings', 'are', 'fighting', 'and', 'your', '##e', 'just', 'watching', 'like']\n",
      "['he', 'was', 'a', 'god', 'im', 'good', 'man', 'and', 'a', 'gonna', 'miss', 'him', 'better', 'friend', 'so', 'much', 'here', '##s', 'to', 'you', 'greg']\n",
      "['when', 'i', 'waved', 'at', 'a', 'friend', 'and', 'they', 'didn', '##t', 'see']\n",
      "['therapist', 'why', 'do', 'you', 'have', 'such', 'a', 'hard', 'time', 'making', 'friends', 'me', 'when', 'someone', 'smiles', 'at', 'me', 'all', 'i', 'see', 'is', 'a', 'chi', '##mp', '##an', '##zee', 'begging', 'for', 'its', 'life']\n",
      "['girls', 'using', 'the', 'guy', 'filter', 'on', 'snap', '##cha', '##t', 'guys', 'using', 'the', 'girl', 'filter', 'on', 'snap', '##cha', '##t']\n",
      "['tooth', '##brush', '##arm', '##y', '·', 'follow', '•', 'me', 'laughing', 'at', 'corona', '##virus', 'me', '##mes', 'laughing', 'me', 'still', 'having', 'to', 'show', 'up', 'to', 'work', 'actually', 'concerned', 'about', 'corona', '##virus', 'crying']\n",
      "['people', 'on', 'the', 'internet', 'oh', 'you', 'can', 'count', 'good', 'for', 'you', 'people', 'from', 'nevada', 'and', 'you', 'can', 'count', 'on', 'me', 'waiting', 'for', 'you', 'in', 'the', 'parking', 'lot']\n",
      "['me', 'trying', 'to', 'grab', 'the', 'last', 'pea', 'on', 'my', 'plate', 'with', 'my', 'fork']\n",
      "['come', 'on', 'jesus', 'show', 'us', 'yo', 'ti', '##tti', '##es', 'im', 'shy', 'lo', '##l']\n",
      "['sleeping', 'for', '2', 'hours', 'sleeping', 'for', '12', 'hours', 'still', 'feeling', 'sleepy']\n",
      "['everyone', 'wants', '##to', 'bea', '##hea', '##the', '##n', 'until', 'its', 'time', 'to', 'do', 'heath', '##en', 'shit', 'then', 'its', 'all', 'violence', 'is', 'scary', 'and', 'mean', 'words', 'hurt', 'my', 'feelings', 'im', '##gf', 'ip']\n",
      "['it', '##res', '##pass', '##ed', 'on', 'government', 'property', 'last', 'night', 'т', '##o', 'practice', 'parallel', 'parking']\n",
      "['my', 'dad', 'who', '##s', 'trying', 'not', 'to', 'show', 'he', '##s', 'really', 'proud', 'me', 'playing', 'ever', '##long', 'on', 'guitar']\n",
      "['teacher', 'for', 'this', 'assignment', 'you', 'may', 'choose', 'your', 'own', 'part', '##ne', 'me', 'and', 'my', 'one', 'friend', 'in', 'the', 'class', '100']\n",
      "['when', 'u', 'buy', 'a', 'game', 'but', 'the', 'next', 'day', 'it', 'is', '50', 'off', 'im', 'never', 'going', 'to', 'financially', 'recover', 'from', 'this']\n",
      "['when', 'your', '##e', 'closing', 'apps', 'and', 'accidentally', 'close', 'the', 'music']\n",
      "['hey', 'its', '##a', 'the', 'nazi', 'bros', 'mama', 'why', '##a', 'you', 'never', 'remember', 'my', 'ideology', 'im', '##a', 'sorry', 'it', '##al', '##jan', 'nazi', 'mama', '##mia']\n",
      "['me', 'im', 'terrified', 'of', 'the', 'cold', 'therapist', 'i', 'see', 'me', 'trembling', 'did', 'you', 'just', 'say', 'icy', 'therapist', 'no', 'chill', 'out', 'me', 'screams']\n",
      "['teachers', 'be', 'like', 'the', 'exam', 'was', 'easy', 'right']\n",
      "['teacher', 'the', 'lowest', 'grade', 'on', 'the', 'test', 'was', 'a', '26', 'me', 'l', '##ma', '##o', 'what', 'dumb', '##ass', 'got', 'a', '26', 'gets', 'test', 'back']\n",
      "['when', 'you', 'change', 'your', 'writing', 'style', 'halfway', 'through', 'the', 'story']\n",
      "['me', 'posts', 'a', 'me', '##me', 'other', 'me', '##me', 'pages', 'our', 'me', '##me']\n",
      "['formats', 'o', 'test', 'an', 'text', 'im', 'text', 'on', 'image', 'c', 'text', 'an', 'video', 'o']\n",
      "['joy', 'reid', 'o', 'these', 'trauma', '##s', 'start', 'early', 'and', 'run', 'deep', 'this', 'little', 'girl', 'is', 'afraid', 'of', 'what', 'she', 'already', 'knows', 'not', 'some', 'bog', '##ey', '##man', 'ah', '##e', 'just', 'imagine']\n",
      "['we', 'want', 'him', 'back', 'we', 'want', 'johnny', 'de', '##pp', 'back', 'as', 'captain', 'jack', 'sparrow', '229', '##75', '##4', 'have', 'signed', 'lets', 'get', 'to', '3000', '##00', 'your', '##e', 'goddamn', 'right']\n",
      "['teacher', 'there', 'are', 'five', 'senses', 'vision', 'hearing', 'smell', 'taste', 'and', 'touch', 'me', 'i', 'can', 'hear', 'image', 'teacher', 'no', 'you', 'can', '##t', 'also', 'me']\n",
      "['me', 'at', '10', 'pm', 'on', 'a', 'friday', 'i', 'promise', 'll', '##l', 'fix', 'my', 'sleep', 'schedule', 'by', 'going', 'to', 'sleep', 'now', 'me', 'on', 'youtube', 'at', '3', 'am', 'you', 'can', 'have', 'this']\n",
      "['teacher', 'class', 'were', 'going', 'to', 'the', 'uk', 'girls', 'oh', 'yes', 'shopping', 'london', 'eye', 'big', 'ben', 'boys']\n",
      "['remember', 'what', 'they', 'did', 'april', '19', '1993']\n",
      "['when', 'your', 'boss', 'says', 'your', '##e', 'paid', 'fairly', '1759', 'lo', '##ase', 'co', '##s', 'o', 'and', '120', '##a', 'c', 'f', '##om', 'pa', 'de', '21', '4', '##k', '##1', 'k', 'k', '##°', 'and']\n",
      "['how', 'people', 'draw', 'me', 'an', 'birds', 'intellectual']\n",
      "['you', 'were', 'like', 'a', 'brother', 'to', 'me', 'i', 'loved', 'you', 'ا', '##ن', '##ا', 'ا', '##ح', '##ب', '##ك', 'ا', '##ي', '##ض', '##ا', 'ا', '##ل', '##ن', '##ر', '##و', '##ي', '##ج']\n",
      "['my', 'un', '##con', '##ditional', 'love', 'and', 'support', 'my', 'friend', 'who', 'has', 'been', 'going', 'through', 'a', 'tough', 'time']\n",
      "['starts', 'milk', '##ing', 'a', 'cow', 'its', 'a', 'bull', 'heard', '##ed', '##gin', '##k']\n",
      "['when', 'you', 'finish', 'a', 'test', 'that', 'the', 'other', 'class', 'hasn', '##t', 'taken', 'yet', 'up', '##o', 'adi', '##das', 'mm', '##ov', '##ista', '##rc', '##ru', '##z', '##c', 'ru', '##z', '##camp', '##o', 'ib', '##er']\n",
      "['task', 'failed', 'successfully', 'ok', 'hold', 'up']\n",
      "['you', 'hear', 'a', 'noise', 'so', 'you', 'walk', 'out', 'and', 'see', 'this', 'w', '##yd']\n",
      "['why', 'you', 'shouldn', '##t', 'wait', 'until', 'retirement', 'to', 'travel']\n",
      "['its', '5', 'but', 'for', 'you', '495', 'my', 'friend', '40', 'i', 'love', 'the', 'internet', 'lo', '##l']\n",
      "['monk', 'just', 'a', 'normal', 'letter', 'm', 'please', 'scribe']\n",
      "['when', 'ya', 'mom', 'say', 'she', 'lea', '##vin', 'you', 'hear', 'the', 'door', 'close', 'but', 'she', 'come', 'back', 'after', '2', 'seconds', 'because', 'she', 'forgot', 'her', 'wallet']\n",
      "['comedic', 'genius', '##es', 'commenting', 'who', '##s', 'here', 'in', '2021', 'on', 'every', 'youtube', 'video']\n"
     ]
    }
   ],
   "source": [
    "for caption_ids in batch_toks['input_ids'].tolist():\n",
    "    print(tokenizer.convert_ids_to_tokens(caption_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 1, 1, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [0, 1, 1, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
